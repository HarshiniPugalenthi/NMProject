Abstract:

This project presents the design and implementation of an end-to-end speech recognition system leveraging deep learning techniques. Traditional speech recognition systems rely on complex pipelines involving separate components such as acoustic models, language models, and pronunciation dictionaries. In contrast, our approach employs a unified neural network architecture that directly maps raw audio waveforms or spectrogram features to transcribed text. We explore state-of-the-art models including Connectionist Temporal Classification (CTC), Sequence-to-Sequence (Seq2Seq) with attention, and Transformer-based architectures. The system is trained and evaluated on publicly available datasets such as LibriSpeech, demonstrating competitive performance in terms of Word Error Rate (WER). Key components of the pipeline include data preprocessing, model training, decoding, and performance evaluation. This work underscores the potential of end-to-end models to simplify speech recognition systems while maintaining or improving transcription accuracy.
